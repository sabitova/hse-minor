{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from string import punctuation\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import scipy.stats as sts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. Составьте самостоятельно как минимум две коллекции текстов разных стилей (например, коллекция текстов в публицистическом стиле и коллекция текстов в научном стиле). Коллекции текстов должны быть достаточно большие (порядка 5000 токенов). Посчитайте количество токенов и типов в каждой коллекции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция read_file считывает текстовый файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    f = open(file)\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция tokenize выделяет токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    exclude = set(punctuation + '0123456789\\n'+u'–—'+u'«»')\n",
    "    merged_text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    tokens = WhitespaceTokenizer().tokenize(merged_text.lower())\n",
    "    for i in tokens[:10]: \n",
    "        print(i)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция print_numbers подсчитывает количество токенов и типов в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_numbers(tokens):\n",
    "    print('Число токенов:', len(tokens))\n",
    "    types = nltk.FreqDist(tokens)\n",
    "    print('Число типов:', len(types))\n",
    "    print(types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для анализа были выбраны три текста: \"Преступление и наказание\" Ф.М. Достоевского (художественный стиль), Налоговый кодекс РФ (официально-деловой стиль) и кандидатская диссертация К.Р. Абаноковой на тему \"Изменение структуры домохозяйства как стратегия преодоления макроэкономического шока\" (научный стиль).\n",
    "\n",
    "Считаем тексты из файлов и подсчитаем количество токенов и типов в каждом из них. \n",
    "\n",
    "Художественный стиль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в\n",
      "начале\n",
      "июля\n",
      "в\n",
      "чрезвычайно\n",
      "жаркое\n",
      "время\n",
      "под\n",
      "вечер\n",
      "один\n"
     ]
    }
   ],
   "source": [
    "text_art = read_file('dostoevsky.txt')\n",
    "tokens_art = tokenize(text_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число токенов: 42088\n",
      "Число типов: 10332\n",
      "<FreqDist with 10332 samples and 42088 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print_numbers(tokens_art)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Научный стиль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "введение\n",
      "актуальность\n",
      "за\n",
      "последние\n",
      "два\n",
      "десятка\n",
      "лет\n",
      "российские\n",
      "домохозяйства\n",
      "испытали\n"
     ]
    }
   ],
   "source": [
    "text_sci = read_file('science.txt')\n",
    "tokens_sci = tokenize(text_sci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число токенов: 6070\n",
      "Число типов: 1999\n",
      "<FreqDist with 1999 samples and 6070 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print_numbers(tokens_sci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Официально-деловой стиль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "часть\n",
      "первая\n",
      "принята\n",
      "государственной\n",
      "думой\n",
      "июля\n",
      "года\n",
      "одобрена\n",
      "советом\n",
      "федерации\n"
     ]
    }
   ],
   "source": [
    "text_ofi = read_file('taxes_law.txt')\n",
    "tokens_ofi = tokenize(text_ofi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число токенов: 28224\n",
      "Число типов: 3649\n",
      "<FreqDist with 3649 samples and 28224 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print_numbers(tokens_ofi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Используя любой морфологический процессор, который вам нравится (pymorphy2, mystem), определите к какой части речи относятся слова из каждой коллекции текстов. При помощи nltk.FreqDist() составьте частотные словари: часть речи – количество слов, к ней относящихся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция pos_freq фильтрует стоп-слова, а также определяет части речи, к которым они относятся, при этом возвращая FreqDist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "\n",
    "def pos_freq(tokens):\n",
    "    #lemmas = [] # список лемм\n",
    "    filtered_text = [] # список для хранения отфильтрованного текста\n",
    "    pos = [] # список частей речи\n",
    "    for word in tokens:\n",
    "        if not word in stopwords.words('russian'):\n",
    "            filtered_text.append(word)\n",
    "            #lemmas.append(morph.parse(word)[0].normal_form)\n",
    "            pos.append(morph.parse(word)[0].tag.POS)   \n",
    "    for i in range(20):\n",
    "        print(filtered_text[i] + ':' + pos[i])\n",
    "    return nltk.FreqDist(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "начале:NOUN\n",
      "июля:NOUN\n",
      "чрезвычайно:ADVB\n",
      "жаркое:NOUN\n",
      "время:NOUN\n",
      "вечер:NOUN\n",
      "молодой:NOUN\n",
      "человек:NOUN\n",
      "вышел:VERB\n",
      "своей:ADJF\n",
      "каморки:NOUN\n",
      "которую:ADJF\n",
      "нанимал:VERB\n",
      "жильцов:NOUN\n",
      "переулке:NOUN\n",
      "улицу:NOUN\n",
      "медленно:ADVB\n",
      "нерешимости:NOUN\n",
      "отправился:VERB\n",
      "мосту:NOUN\n"
     ]
    }
   ],
   "source": [
    "POS_freq_art = pos_freq(tokens_art)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распределение слов по частям речи в тексте художественного стиля:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'ADJF': 3487,\n",
       "          'ADJS': 519,\n",
       "          'ADVB': 2047,\n",
       "          'COMP': 145,\n",
       "          'CONJ': 401,\n",
       "          'GRND': 467,\n",
       "          'INFN': 1000,\n",
       "          'INTJ': 71,\n",
       "          'NOUN': 8535,\n",
       "          'NPRO': 284,\n",
       "          'NUMR': 259,\n",
       "          'PRCL': 436,\n",
       "          'PRED': 34,\n",
       "          'PREP': 133,\n",
       "          'PRTF': 302,\n",
       "          'PRTS': 254,\n",
       "          'VERB': 5072})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_freq_art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Научный стиль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "введение:NOUN\n",
      "актуальность:NOUN\n",
      "последние:ADJF\n",
      "десятка:NOUN\n",
      "лет:NOUN\n",
      "российские:ADJF\n",
      "домохозяйства:NOUN\n",
      "испытали:VERB\n",
      "несколько:ADVB\n",
      "экономических:ADJF\n",
      "шоков:ADJS\n",
      "повлекших:PRTF\n",
      "собой:NPRO\n",
      "части:NOUN\n",
      "снижение:NOUN\n",
      "благосостояния:NOUN\n",
      "кризис:NOUN\n",
      "года:NOUN\n",
      "привел:VERB\n",
      "обесцениванию:NOUN\n"
     ]
    }
   ],
   "source": [
    "POS_freq_sci = pos_freq(tokens_sci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'ADJF': 932,\n",
       "          'ADJS': 35,\n",
       "          'ADVB': 71,\n",
       "          'COMP': 2,\n",
       "          'CONJ': 62,\n",
       "          'GRND': 24,\n",
       "          'INFN': 132,\n",
       "          'NOUN': 2759,\n",
       "          'NPRO': 8,\n",
       "          'NUMR': 10,\n",
       "          'PRCL': 10,\n",
       "          'PREP': 35,\n",
       "          'PRTF': 147,\n",
       "          'PRTS': 50,\n",
       "          'VERB': 379})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_freq_sci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Официально-деловой стиль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "часть:NOUN\n",
      "первая:ADJF\n",
      "принята:PRTS\n",
      "государственной:ADJF\n",
      "думой:NOUN\n",
      "июля:NOUN\n",
      "года:NOUN\n",
      "одобрена:PRTS\n",
      "советом:NOUN\n",
      "федерации:NOUN\n",
      "июля:NOUN\n",
      "года:NOUN\n",
      "раздел:NOUN\n",
      "общие:ADJF\n",
      "положения:NOUN\n",
      "глава:NOUN\n",
      "законодательство:NOUN\n",
      "налогах:NOUN\n",
      "сборах:NOUN\n",
      "иные:ADJF\n"
     ]
    }
   ],
   "source": [
    "POS_freq_ofi = pos_freq(tokens_ofi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'ADJF': 5672,\n",
       "          'ADJS': 52,\n",
       "          'ADVB': 239,\n",
       "          'CONJ': 166,\n",
       "          'GRND': 14,\n",
       "          'INFN': 272,\n",
       "          'NOUN': 13477,\n",
       "          'NPRO': 31,\n",
       "          'NUMR': 60,\n",
       "          'PRCL': 17,\n",
       "          'PRED': 32,\n",
       "          'PRTF': 833,\n",
       "          'PRTS': 238,\n",
       "          'VERB': 883})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_freq_ofi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# del POS_freq_ofi['PREP'] - так можно просто удалить "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Увидели, что все равно остались служебные части речи. Есть два пути. Первый - добавить их все в список стоп-слов, \n",
    "# а потом заново запустить все, что было выше. Вообще, наверное, это стоило сделать в самом начале, я уж не буду сейчас этого\n",
    "# делать.\n",
    "# PREP - предлог, CONJ - союз, NPRO - местоимение, PRCL - частица"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mystopwords = stopwords.words('russian')\n",
    "for word in tokens_art:\n",
    "    if morph.parse(word)[0].tag.POS == 'PREP' or morph.parse(word)[0].tag.POS == 'CONJ' or morph.parse(word)[0].tag.POS == 'NPRO' or morph.parse(word)[0].tag.POS == 'PRCL':\n",
    "        if not word in mystopwords:\n",
    "            mystopwords.append(word)\n",
    "            \n",
    "for word in tokens_sci:\n",
    "    if morph.parse(word)[0].tag.POS == 'PREP' or morph.parse(word)[0].tag.POS == 'CONJ' or morph.parse(word)[0].tag.POS == 'NPRO' or morph.parse(word)[0].tag.POS == 'PRCL':\n",
    "        if not word in mystopwords:\n",
    "            mystopwords.append(word)\n",
    "            \n",
    "for word in tokens_ofi:\n",
    "    if morph.parse(word)[0].tag.POS == 'PREP' or morph.parse(word)[0].tag.POS == 'CONJ' or morph.parse(word)[0].tag.POS == 'NPRO' or morph.parse(word)[0].tag.POS == 'PRCL':\n",
    "        if not word in mystopwords:\n",
    "            mystopwords.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы подсчитать корреляцию Спирмена, необходимо сначала составить список всех частей речи, которые встретились в трех текстах (кроме служебных), а затем для каждого текста каждой части речи сопоставить число слов, принадлежащих этой части речи и входящих к текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# записываем все возможные части речи в один список\n",
    "a = list(POS_freq_sci.keys())\n",
    "a.extend(list(POS_freq_ofi.keys()))\n",
    "a.extend(list(POS_freq_art.keys()))\n",
    "a = set(a) # делаем из списка множество и удаляем из него PREP, CONJ, NPRO, PRCL\n",
    "a.remove('PREP')\n",
    "a.remove('CONJ')\n",
    "a.remove('NPRO')\n",
    "a.remove('PRCL')\n",
    "a = list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# списки для хранения векторов чисел, соответствующих частям речи списка a\n",
    "art = []\n",
    "sci = []\n",
    "ofi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in a:\n",
    "    art.append(POS_freq_art[k])\n",
    "    sci.append(POS_freq_sci[k])\n",
    "    ofi.append(POS_freq_ofi[k])\n",
    "\n",
    "# теперь в списках art, sci, ofi лежат упорядоченные значения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Подсчитаем все возможные парные корреляции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.81155510062407588, pvalue=0.00075652329921874391)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.spearmanr(art, ofi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.88858405932737805, pvalue=4.9102393737752382e-05)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.spearmanr(art, sci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.95179063360881544, pvalue=5.546738050422406e-07)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.spearmanr(sci, ofi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все p-value оказались достаточно малы (меньше 0.01), а это значит, что на уровне значимости 0.01 можно отвергнуть основную гипотезу, которая заключается в том, что корреляция равна 0.\n",
    "\n",
    "Можно наблюдать, что наиболее сильно коррелируют тексты научного и официального стиля, в то время как каждый из них коррелирует с текстом художественного стиля не так сильно, т.е. научный и официальный стиль похожи друг на друга больше, чем каждый из них похож на художественный."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
